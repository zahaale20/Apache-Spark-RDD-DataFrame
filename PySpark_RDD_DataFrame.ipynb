{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoolY63aauhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1258208-0bff-41b5-aa0b-5fa4b6fff0c3"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=d4274b3bd44a42d67f9567d824e02201df56bb97f62314df9ed5d2226924a0f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 8 not upgraded.\n",
            "Need to get 39.7 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 120509 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u372-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u372-ga~us1-0ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u372-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u372-ga~us1-0ubuntu1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tad7fmwTbMwW"
      },
      "source": [
        "\n",
        "# Imports / Starter Example\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKUgYFX9olK5"
      },
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, SQLContext\n",
        "from pyspark.sql import types as sparktypes\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "sc = SparkContext()\n",
        "spark = SparkSession(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slRw9-jLsHOd"
      },
      "source": [
        "# download a sample access log for use in demos below\n",
        "!rm -f apache.access.log\n",
        "!wget -q https://raw.githubusercontent.com/databricks/reference-apps/master/logs_analyzer/data/apache.access.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlPvNuJGgkQl"
      },
      "source": [
        "# (A) RDD Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using RDD transformations\n",
        "\n",
        "[RDD APIs PySpark v3.3.0](https://spark.apache.org/docs/3.3.0/api/python/reference/pyspark.html#rdd-apis)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzNTpq69g1pU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2294d7d-88e0-4f43-dbaf-0784c7831856"
      },
      "source": [
        "# RDD implementation\n",
        "# (1) Most popular URL paths (top 15)\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[6], 1 ))\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "                  .sortBy(lambda t: -t[1]))\n",
        "\n",
        "print (\"Top 15 URL Paths:\")\n",
        "for line in access_log_rdd.take(15):\n",
        "  print(line[0], \", \", line[1], sep=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 15 URL Paths:\n",
            "/twiki/bin/view/Main/WebHome, 40\n",
            "/twiki/pub/TWiki/TWikiLogos/twikiRobot46x50.gif, 32\n",
            "/, 31\n",
            "/favicon.ico, 28\n",
            "/robots.txt, 27\n",
            "/razor.html, 23\n",
            "/twiki/bin/view/Main/SpamAssassinTaggingOnly, 18\n",
            "/twiki/bin/view/Main/SpamAssassinAndPostFix, 17\n",
            "/cgi-bin/mailgraph2.cgi, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_0.png, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_1_err.png, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_1.png, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_0_err.png, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_2.png, 16\n",
            "/cgi-bin/mailgraph.cgi/mailgraph_2_err.png, 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az250Dgghvd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3190ce30-4e24-4fa8-9866-62d6dbbbea76"
      },
      "source": [
        "# RDD implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: ( line.split(\" \")[8], 1 ))\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "                  .sortBy(lambda t: t[0]))\n",
        "\n",
        "print (\"Count for each HTTP response Code (sorted ascending order by response code):\")\n",
        "for line in access_log_rdd.collect():\n",
        "  print(line[0], \", \", line[1], sep=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count for each HTTP response Code (sorted ascending order by response code):\n",
            "200, 1272\n",
            "302, 6\n",
            "401, 123\n",
            "404, 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa17VZSyhv4X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb1625b-bebe-48b7-b674-afca7a918d84"
      },
      "source": [
        "# RDD implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .map(lambda line: (line.split(\" \")[3][4:12], 1))  # Extracting only the month and year\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "                  .sortBy(lambda t: t[0]))\n",
        "\n",
        "print(\"Count for each calendar month and year, sorted chronologically:\")\n",
        "for line in access_log_rdd.collect():\n",
        "  print(line[0].replace(\"/\", \" \"), \", \", line[1], sep=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count for each calendar month and year, sorted chronologically:\n",
            "Mar 2004, 1406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSUdQb4Mhv-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b12c7f5a-a642-436d-cd47-28e1d0c6519c"
      },
      "source": [
        "# RDD implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "def selectClient(line):\n",
        "    client = \"145.253.208.9\"\n",
        "    if client in line:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .filter(selectClient)\n",
        "                  .map(lambda line: (line.split(\" \")[0], int(line.split(\" \")[9])))\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2))\n",
        "\n",
        "print(\"Total bytes sent to the client with the specified hostname or IPv4 address:\")\n",
        "for line in access_log_rdd.collect():\n",
        "  print(line[0], \", \", line[1], sep=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total bytes sent to the client with the specified hostname or IPv4 address:\n",
            "145.253.208.9, 26098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi89G8EQhwEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f76cdc-f42a-4f58-d00a-b4997f0f63ba"
      },
      "source": [
        "# RDD implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n",
        "def selectURL(line):\n",
        "    url = \"/robots.txt\"\n",
        "    if url in line:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "access_log_rdd = (sc.textFile(\"apache.access.log\")\n",
        "                  .filter(selectURL)\n",
        "                  .map(lambda line: (line.split(\" \")[0], 1))\n",
        "                  .reduceByKey(lambda count1, count2: count1 + count2)\n",
        "                  .sortBy(lambda t: t[1], ascending=False))\n",
        "\n",
        "print(\"Request Count for each client of a given URL\")\n",
        "for line in access_log_rdd.collect():\n",
        "  print(line[0], \", \", line[1], sep=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Request Count for each client of a given URL\n",
            "lj1024.inktomisearch.com, 8\n",
            "lj1048.inktomisearch.com, 5\n",
            "lj1036.inktomisearch.com, 5\n",
            "mmscrm07-2.sac.overture.com, 3\n",
            "64.242.88.10, 2\n",
            "lj1052.inktomisearch.com, 1\n",
            "cr020r01-3.sac.overture.com, 1\n",
            "crawl24-public.alexa.com, 1\n",
            "lj1007.inktomisearch.com, 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxkjTvxIhBO-"
      },
      "source": [
        "# (B) DataFrame Implementations\n",
        "\n",
        "Perform reporting tasks 1-5 using Spark's DataFrame API\n",
        "\n",
        "[DataFrame API PySpark v.3.1.1](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame)\n",
        "\n",
        "Please Note: Spark SQL (`createOrReplaceTempView(...)`, `spark.sql(\"SELECT * ...\")`) **is not** permitted for these exercises. You must use the Spark DataFrame API. You may, however, use the SQL functions defined in [`pyspark.sql.functions`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "access_log_df = spark.read.options(delimiter=\" \").csv(\"apache.access.log\")\n",
        "\n",
        "named_df = access_log_df.select(col('_c0').alias('host'),\n",
        "                                col('_c3').alias('timestamp'),\n",
        "                                col('_c5').alias('http_request_line'),\n",
        "                                col('_c6').cast('integer').alias('status'),\n",
        "                                col('_c7').cast('integer').alias('content_size'))"
      ],
      "metadata": {
        "id": "-jyHatYU-mQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhGYTdWLhJCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aec074e-dacc-4f94-ce35-2af2846ef367"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (1) Most popular URL paths (top 15)\n",
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "named_df.select(regexp_replace(regexp_replace(named_df[\"http_request_line\"], \"^GET \", \"\"), \" HTTP/\\\\d\\\\.\\\\d$\", \"\")\n",
        "  .alias(\"http_request\")) \\\n",
        "  .groupby(\"http_request\") \\\n",
        "  .count() \\\n",
        "  .orderBy(\"count\", ascending=False) \\\n",
        "  .limit(15) \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|        http_request|count|\n",
            "+--------------------+-----+\n",
            "|/twiki/bin/view/M...|   40|\n",
            "|/twiki/pub/TWiki/...|   32|\n",
            "|                   /|   31|\n",
            "|        /favicon.ico|   28|\n",
            "|         /robots.txt|   27|\n",
            "|         /razor.html|   23|\n",
            "|/twiki/bin/view/M...|   18|\n",
            "|/twiki/bin/view/M...|   17|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "|/cgi-bin/mailgrap...|   16|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apq4hUSVh94J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dfa342e-d4a0-45d6-dfda-0ab2aa62113b"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (2) Request count for each HTTP response code, sorted by response code\n",
        "named_df.select(\"status\") \\\n",
        "  .groupby(\"status\") \\\n",
        "  .count() \\\n",
        "  .orderBy(\"status\") \\\n",
        "  .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|status|count|\n",
            "+------+-----+\n",
            "|   200| 1272|\n",
            "|   302|    6|\n",
            "|   401|  123|\n",
            "|   404|    5|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSvjvCNth9-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee432cf2-f2e4-46c5-96c3-c0e0df2401e5"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (3) Request count for each calendar month and year, sorted chronologically\n",
        "from pyspark.sql.functions import year, month, to_date\n",
        "\n",
        "# Assuming \"timestamp\" is a string column\n",
        "result_df = named_df.withColumn(\"date\", to_date(named_df[\"timestamp\"].substr(2, 20), \"dd/MMM/yyyy:HH:mm:ss\")) \\\n",
        "                    .withColumn(\"year\", year(\"date\")) \\\n",
        "                    .withColumn(\"month\", month(\"date\")) \\\n",
        "                    .groupBy(\"year\", \"month\") \\\n",
        "                    .count() \\\n",
        "                    .orderBy(\"year\", \"month\")\n",
        "\n",
        "result_df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----+-----+\n",
            "|year|month|count|\n",
            "+----+-----+-----+\n",
            "|2004|    3| 1406|\n",
            "+----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjkoHWOxh-Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4710162f-c171-42b2-b6ee-0ed5876ab37e"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (4) Total bytes sent to the client with a specified hostname or IPv4 address (you may hard code an address)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "named_df.select(\"host\", \"content_size\") \\\n",
        "                        .where(col(\"host\") == \"145.253.208.9\") \\\n",
        "                        .groupby(\"host\") \\\n",
        "                        .sum(\"content_size\") \\\n",
        "                        .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----------------+\n",
            "|         host|sum(content_size)|\n",
            "+-------------+-----------------+\n",
            "|145.253.208.9|            26098|\n",
            "+-------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiwpDYxTh-Iv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fed2419-0511-44a9-c0b0-899c5b31ebd9"
      },
      "source": [
        "# DataFrame implementation\n",
        "# (5) Based on a given URL (hard coded), compute a request count for each client (hostname or IPv4) who accessed that URL, sorted by request count, highest to lowest\n",
        "named_df.select(\"host\") \\\n",
        "                           .where(col(\"http_request_line\").contains(\"/robots.txt\")) \\\n",
        "                           .groupby(\"host\") \\\n",
        "                           .count() \\\n",
        "                           .orderBy(col(\"count\").desc()) \\\n",
        "                           .show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|                host|count|\n",
            "+--------------------+-----+\n",
            "|lj1024.inktomisea...|    8|\n",
            "|lj1048.inktomisea...|    5|\n",
            "|lj1036.inktomisea...|    5|\n",
            "|mmscrm07-2.sac.ov...|    3|\n",
            "|        64.242.88.10|    2|\n",
            "|cr020r01-3.sac.ov...|    1|\n",
            "|crawl24-public.al...|    1|\n",
            "|lj1052.inktomisea...|    1|\n",
            "|lj1007.inktomisea...|    1|\n",
            "+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}